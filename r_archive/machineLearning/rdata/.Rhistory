corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
lapply(corpus, as.character)
lapply(corpus_clean, as.character)
sms_dtm <- TermDocumentMatrix(corpus, control= list(weighting=weightTfIdf))
inspect(sms_dtm)
sms_dtm3 <- TermDocumentMatrix(corpus)
sms_dtm3
inspect(sms_dtm3)
findFreqTerms(TermDocumentMatrix(corpus), lowfreq=2)
findAssocs(TermDocumentMatrix(corpus),"user", 0.5)
sms_dtm2 <- DocumentTermMatrix(corpus, control = list(weighting = weightTfIdf))
inspect(sms_dtm2)
library(caret)
install.packages("caret")
install.packages("rpart")
library(caret)
library(rpart)
library(nnet)
sms_dtm2_df <- cbind(as.data.frame(as.matrix(sms_dmt2)), LABEL = docs$type )
sms_dtm2_df <- cbind(as.data.frame(as.matrix(sms_dtm2)), LABEL = docs$type )
sms_dtm2_df
m <- nnet(LABEL ~. , data = sms_dtm2_df, size=3)
predict(m, newdata = sms_dtm2_df)
predict(m, newdata = sms_dtm2_df)
m <- nnet(LABEL ~. , data = sms_dtm2_df, size=3)
predict(m, newdata = sms_dtm2_df)
advice <- read.csv("advice.csv", header=T, stringsAsFactors = F)
advice
open(advice)
open("advice.csv"
)
place <- sapply(advice[,2], extractNoum, USE.NAMES = F)
place <- sapply(advice[,2], extractNoun, USE.NAMES = F)
place
c <- unlist(place)
c
place2 <- Filter(function(x), {nchar(x) >= 2 & nchar(x) <= 5)}, c)
place2 <- Filter(function(x) {nchar(x) >= 2 & nchar(x) <= 5)}, c)
place2 <- Filter(function(x) {nchar(x) >= 2 & nchar(x) <= 5},c)
place2
res <- str_replace_all(place2, "[^[:alpha:]]" , "")
library("stringr")
res <- str_replace_all(place2, "[^[:alpha:]]" , "")
res
res <- res[res != ""]
res
wordcount <- table(res)
wordcount
wordcount2 <- sort(table(res), decreasing = T)
wordcount2
library(wordcloud)
install.packages("wordcloud")
install.packages("RColorBrewer")
palete <- brewer.pal(8, "Set2")
library(wordcloud)
library(RColorBrewer)
palete <- brewer.pal(8, "Set2")
palete
wordcloud(names(wordcount), freq = wordcount , scale=c(3,1), rot.per = 0.25, min.freq = 1, random.order = F, random.color = T, colors = palete)
c(3,1)
keyword <- dimnames(wordcount2[1:10])$res
keyword
keyword <- dimnames(wordcount2[1:10])
keyword
keyword <- dimnames(wordcount2[1:10])$res
keyword
contents <- c()
contents
for (i in 1:6){
inter <- intersect(place[[i]], keyword)
contents <- rbind(contents, table(inter)[keyword])
}
contents
inter
rownames(contents) <- advice$DATE
colname(contents) <- keyword
colnames(contents) <- keyword
contents
contents[which(is.na(contents))] <- 0
contents
advice2 <- read.csv("advice2.csv", header =T ,stringsAsFactors = F)
rownames(advice2) <- advice2[,1]
advice2
advice2 <- advice2[-1]
head(advice2)
advice3 <- ifelse(advice2 > mean(apply(advice2, 2, mean)), 1 ,0)
head(advice3)
library(arules)
trans <- as.matrix(advice3, "Transaction")
trans
head(trans)
rules1 <- apriori(trans, parameter = list(supp=0.3, conf=0.7, target="rules"))
rules1
inspect(sort(rules1))
inspect(sort(rules1))
inspect(rules1)
help(inspect)
arules::inspect(sort(rules1))
rules2 <- subset(rules1, subset = lhs %pin% '초밥' & confidence > 0.7)
rules2
arules::inspect(sort(rules2))
rules2 <- subset(rules1, subset = lhs %pin% '초밥')
arules::inspect(sort(rules2))
image(trans)
library(arulesViz)
install.packages("arulesViz")
library(arulesViz)
plot(rules1)
plot(rules1, method="grouped")
plot(rules1)
plot(rules1, method="grouped")
plot(rules1, method="graph")
plot(rules1, method="graph", control = list(type="items"))
plot(rules1, method="paracoord", control = list(reorder = TRUE)
)
cor(advice2)
library(corrgram)
install.packages("corrgram")
library(corrgram)
corrgram(cor(advice2))
library(corrplot)
install.packages("corrplot")
library(corrplot)
corrplot(cor(advice2))
libcurlVersion(sna)
library(sna)
library(rgl)
advice_square <- t(as.matrix(advice2)) %*% as.matrix(advice2)
advice_square
gplot(sqrt(sqrt(advice_square)), displaylabel = T, vertex.cex = sqrt(diag(advice_square)) * 0.01, label = rownames(advice_square), edge.col = "blue", boxed.labels = F, arrowhead.cex = 0.01, edge.lwd = 0.01, vertex.alpha = 0.01)
gplot(sqrt(sqrt(advice_square)), displaylabel = T, vertex.cex = sqrt(diag(advice_square)) * 0.01, label = rownames(advice_square), edge.col = "blue", vertex.col = "green", boxed.labels = F, arrowhead.cex = 0.01, edge.lwd = 0.01, vertex.alpha = 0.01)
gplot(sqrt(sqrt(advice_square)), displaylabel = T, vertex.cex = sqrt(diag(advice_square)) * 0.01, label = rownames(advice_square), edge.col = "orange", vertex.col = "green", boxed.labels = F, arrowhead.cex = 0.01, edge.lwd = 0.01, vertex.alpha = 0.01)
gplot(sqrt(sqrt(advice_square)), displaylabel = T, vertex.cex = sqrt(diag(advice_square)) * 0.01, label = rownames(advice_square), edge.col = "orange", vertex.col = "green", boxed.labels = F, arrowhead.cex = 0.01, edge.lwd = 0.01)
help(gplot)
library(zoo)
dates <- as.Date(rownames(advice2), format="%Y-%m-%d")
dates
time_keywords <- zoo(advice2, dates)
time_keywords
plot(time_keywords)
help(zoo)
ccf(advice2$고등어, advice2$연어, main = "고등어 vs 연어")
help(ccf)
drink <- read.csv("drink.csv", header = T)
drink
str(drink)
attach(drink)
library(class)
m <-glm(지각여부 ~ 나이 + 결혼여부 + 자녀여부 + 체력 + 주량 + 직급 + 성별, family = binomial(link=logit), data = drink)
m
predict(m, drink, tpye="response")
predict(m, drink, type="response")
predict(m, drink, type="response") >= 0.5)
predict(m, drink, type="response") >= 0.5
table(drink$지각여부, predict(m, drink, type="response") > 0.5)
drink$지각여부
ball <- read.csv("ball.csv", header = T)
ball
str(ball)
library(nnet)
m2 <- multinom(선물 ~ . , data = ball)
m2
cbind(fitted(m2) , levels(ball$선물)[ball$선물])
predicted <- levels(ball$선물)[apply(fitted(m2), 1, which.max)]
predicted
sum(predicted != ball$선물)
xtabs(~ predicted + ball$선물)
test <- read.csv("rule.csv", header = T)
test
P--[[;///]]
r1 <- NROW(subset(test, 효과 == "YES")) / NROW(test)
r1
r2 <- NROW(subset(test, 효과 == "NO")) / NROW(test)
r2
cond1 <- subset(test, 과목 == "수학" & 수업일 == "주말")
cond1
f1 <- NROW(subset(cond1, 효과 == "YES"))
f2 <- NROW(subset(cond1, 효과 == "NO"))
e1 <- NROW(cond1) * r1
f1
f2
e1
r1
r2
e2 <- NROW(cond1) * r2
e2
like11 <- 2 * (f1 * log(f1/e1) + f2 * log(f2/e2))
like1 <- 2 * (f1 * log(f1/e1) + f2 * log(f2/e2))
like1
cove <- NROW(cond1) / NROW(test)
test <- read.csv("rule.csv", header = T)
r1 <- NROW(subset(test, 효과 == "YES")) / NROW(test)
r2 <- NROW(subset(test, 효과 == "NO")) / NROW(test)
like_cov_acc <- function(cond, res)
{
f1 <- NROW(subset(cond1, 효과 == "YES"))
f2 <- NROW(subset(cond1, 효과 == "NO"))
e1 <- NROW(cond1) * r1
e2 <- NROW(cond1) * r2
like <- 2 * (f1 * log(f1/e1) + f2 * log(f2/e2))
# 적용도 coverage와 정확도 accuracy
cove <- NROW(cond1) / NROW(test)
acc <- NROW(subset(cond1, 효과 == "NO")) / NROW(cond1)
cat("적용도는", cove, "입니다\n")
cat("정확도는", acc, "입니다\n")
cat("기능도비율은", like, "입니다\n")
}
# RULE 1
cond1 <- subset(test, 과목 == "수학" & 수업일 == "주말")
res1 = "NO"
like_cov_acc(cond1, res1)
# RULE 2
cond2 <- subset(test, 과목 == "과학")
res2 <- "NO"
like_cov_acc(cond2, res2)
# RULE 3
cond3 <- subset(test, (과목 == "과학" | 과목 == "수학") & 수업시간대 == "저녁" & (class == "A" | classs == "B"))
res3 <- "NO"
like_cov_acc(cond3, res3)
# RULE 4
cond4 <- subset(test, (과목 == "국어" | 과목 == "영어") & (class == "A" | classs == "B"))
res4 <- "NO"
like_cov_acc(cond4, res4)
test <- read.csv("rule.csv", header = T)
r1 <- NROW(subset(test, 효과 == "YES")) / NROW(test)
r2 <- NROW(subset(test, 효과 == "NO")) / NROW(test)
like_cov_acc <- function(cond, res)
{
f1 <- NROW(subset(cond, 효과 == "YES"))
f2 <- NROW(subset(cond, 효과 == "NO"))
e1 <- NROW(cond) * r1
e2 <- NROW(cond) * r2
like <- 2 * (f1 * log(f1/e1) + f2 * log(f2/e2))
# 적용도 coverage와 정확도 accuracy
cove <- NROW(cond) / NROW(test)
acc <- NROW(subset(cond, 효과 == res)) / NROW(cond)
cat("적용도는", cove, "입니다\n")
cat("정확도는", acc, "입니다\n")
cat("기능도비율은", like, "입니다\n")
}
# RULE 1
cond1 <- subset(test, 과목 == "수학" & 수업일 == "주말")
res1 = "NO"
like_cov_acc(cond1, res1)
# RULE 2
cond2 <- subset(test, 과목 == "과학")
res2 <- "NO"
like_cov_acc(cond2, res2)
# RULE 3
cond3 <- subset(test, (과목 == "과학" | 과목 == "수학") & 수업시간대 == "저녁" & (class == "A" | classs == "B"))
res3 <- "NO"
like_cov_acc(cond3, res3)
# RULE 4
cond4 <- subset(test, (과목 == "국어" | 과목 == "영어") & (class == "A" | classs == "B"))
res4 <- "NO"
like_cov_acc(cond4, res4)
cond3 <- subset(test, (과목 == "과학" | 과목 == "수학") & 수업시간대 == "저녁" & (class == "A" | classs == "B"))
test <- read.csv("rule.csv", header = T)
r1 <- NROW(subset(test, 효과 == "YES")) / NROW(test)
r2 <- NROW(subset(test, 효과 == "NO")) / NROW(test)
like_cov_acc <- function(cond, res)
{
f1 <- NROW(subset(cond, 효과 == "YES"))
f2 <- NROW(subset(cond, 효과 == "NO"))
e1 <- NROW(cond) * r1
e2 <- NROW(cond) * r2
like <- 2 * (f1 * log(f1/e1) + f2 * log(f2/e2))
# 적용도 coverage와 정확도 accuracy
cove <- NROW(cond) / NROW(test)
acc <- NROW(subset(cond, 효과 == res)) / NROW(cond)
cat("적용도는", cove, "입니다\n")
cat("정확도는", acc, "입니다\n")
cat("기능도비율은", like, "입니다\n")
}
# RULE 1
cond1 <- subset(test, 과목 == "수학" & 수업일 == "주말")
res1 = "NO"
like_cov_acc(cond1, res1)
# RULE 2
cond2 <- subset(test, 과목 == "과학")
res2 <- "NO"
like_cov_acc(cond2, res2)
# RULE 3
cond3 <- subset(test, (과목 == "과학" | 과목 == "수학") & 수업시간대 == "저녁" & (class == "A" | class == "B"))
res3 <- "NO"
like_cov_acc(cond3, res3)
# RULE 4
cond4 <- subset(test, (과목 == "국어" | 과목 == "영어") & (class == "A" | class == "B"))
res4 <- "NO"
like_cov_acc(cond4, res4)
ls
str(ls)
str(ll)
test <- read.csv("rule.csv", header = T)
r1 <- NROW(subset(test, 효과 == "YES")) / NROW(test)
r2 <- NROW(subset(test, 효과 == "NO")) / NROW(test)
like_cov_acc <- function(cond, res)
{
f1 <- NROW(subset(cond, 효과 == "YES"))
f2 <- NROW(subset(cond, 효과 == "NO"))
e1 <- NROW(cond) * r1
e2 <- NROW(cond) * r2
like <- 2 * (f1 * log(f1/e1) + f2 * log(f2/e2))
# 적용도 coverage와 정확도 accuracy
cove <- NROW(cond) / NROW(test)
acc <- NROW(subset(cond, 효과 == res)) / NROW(cond)
cat("적용도는", cove, "입니다\n")
cat("정확도는", acc, "입니다\n")
cat("기능도비율은", like, "입니다\n")
}
# RULE 1
cond1 <- subset(test, 과목 == "수학" & 수업일 == "주말")
res1 = "NO"
like_cov_acc(cond1, res1)
# RULE 2
cond2 <- subset(test, 과목 == "과학")
res2 <- "NO"
like_cov_acc(cond2, res2)
# RULE 3
cond3 <- subset(test, (과목 == "과학" | 과목 == "수학") & 수업시간대 == "저녁" & (class == "A" | class == "B"))
res3 <- "NO"
like_cov_acc(cond3, res3)
# RULE 4
cond4 <- subset(test, (과목 == "국어" | 과목 == "영어") & (class == "A" | class == "B"))
res4 <- "NO"
like_cov_acc(cond4, res4)
'''
R +
'''
library(sna)
set1 <- read.csv("set1.csv", header = T, stringAsFactors = F)
set1 <- read.csv("set1.csv", header = T, stringsAsFactors = F)
set1
library(MASS)
density <- kde2d(set1$food, set1$book, n = 400)
image(density, xlab="food", ylab="book")
library(e1071)
m1 <- svm(status ~ food + book + cul + cloth + travel, type = "C-classification", data = set1)
m1
predict(m1, set1)
sum(set1$status != predict(m1,set1))
library(kernlab)
m2 <- ksvm(status ~., kernel="rbfdot", data = set1)
m2
set1
predict(m2, set1)
help(as)
sum(as.numeric(predict(m2, set1) > 0.5) != set1$status)
library(e1071)
m1 <- svm(status ~ ., type = "C-classification", data = set1)
predict(m1, set1)
sum(set1$status != predict(m1,set1))
help(zoo)
diff
help(diff)
diff(1:10, 2)
diff(1:10, 2,2)
x <- cumsum(cumsum(1:10))
x
y
y <- cumsum(1:10)
y
diff(x, lag = 2)
x
library(zoo)
library(urca)
cj <- read.zoo("cj.csv", sep = ",", header = TRUE, format = "%Y-%m-%d")
cj2 <- scale(cj)
head(cj2)
simple_mod <- lm(diff(cj2$c_food) ~ diff(cj2$s_food) + 0)
summary(simple_mod)
c_adf <- ur.df(cj2$c_food , type = "drift")
summary(c_adf)
ur.df
install.packages("urca")
library(urca)
c_adf <- ur.df(cj2$c_food , type = "drift")
summary(c_adf)
cj2
head(cj2)
s_adf <- ur.df(cj2$s_food, type = "drift")
summary(s_adf)
help(ur)
help(ur.df)
reg1 <- summary(lm(cj2$c_food ~ cj2$s_food))
error <- residuals(reg1)
error_df <- ur.df(error, type = "none")
summary(error_df)
library(timeSeries)
install.packages("timeSeries")
install.packages("PerformanceAnalytics")
library(timeSeries)
library(PerformanceAnalytics)
CJ <- timeSeries(cj_df, rownames(cj_df))
library(timeSeries)
library(PerformanceAnalytics)
CJ <- timeSeries(cj_df, rownames(cj_df))
CJ_return <- returns(CJ)
x11(); chart.CumReturns(CJ_return, legend.loc = "topleft", main="")
cj_df <- as.data.frame(cj)
return <- log(tail(cj_df, -1) / head(cj_df, -1))
Q <- cov(return)
n <- ncol(cj_df)
r <- colMeans(cj_df)
Q1 <- rbind(Q, rep(1,n), r)
Q2 <- cbind(Q1, rbind(t(tail(Q1, 2)) , matrix(0,2,2)))
rbase <- seq(min(r), max(r), length = 100)
s <- sapply(rbase, function(x)
{
y <- head(solve(Q2, c(rep(0,n), 1, x)), n)
y %*% Q %*% y
})
plot(s, rbase, xlab = "variance", ylab = "Return")
plot(s, rbase, xlab = "variance", ylab = "Return")
CJ <- timeSeries(cj_df, rownames(cj_df))
CJ_return <- returns(CJ)
x11(); chart.CumReturns(CJ_return, legend.loc = "topleft", main="")
install.packages("fPortfolio")
library(fPortfolio)
plot(portfolioFrontier(CJ_return))
food < read.zoo("food2.csv", sep=",", header = TRUE, format = "%Y-%m-%d")
food_df <- as.data.frame(food)
plot(food)
food2 <- scale(food)
sampyo_adf <- ur.df(food2$sampyo, type="drift")
summary(sampyo_adf)
seoul_adf <- ur.df(food2$seoul, type="drift")
summary(seoul_adf)
food2
sampyo_adf <- ur.df(food2$sampyo, type="drift")
summary(sampyo_adf)
seoul_adf <- ur.df(food2$seoul, type="drift")
summary(seoul_adf)
food2$sampyo
food <- read.zoo("food2.csv", sep=",", header = TRUE, format = "%Y-%m-%d")
food_df <- as.data.frame(food)
plot(food)
food2 <- scale(food)
sampyo_adf <- ur.df(food2$sampyo, type="drift")
summary(sampyo_adf)
seoul_adf <- ur.df(food2$seoul, type="drift")
summary(seoul_adf)
reg2 <- summary(lm(food2$sampyo ~ food2$seoul))
error2 <- residuals(reg2)
error_df2 <- ur.df(error2, type="none")
summary(error_df2)
diff(1,2)
diff(1,20)
diff(1:100,2)
diff(10, 20)
diff(10:15, 20)
diff(1:15, 20)
diff(1:2, 20)
diff(1:2, 2)
diff(1:10, 10)
diff(1:10, 9)
food <- read.zoo("food2.csv", sep=",", header = TRUE, format = "%Y-%m-%d")
food_df <- as.data.frame(food)
plot(food)
food2 <- scale(food)
# sampyo의 비정상성 df 검증
sampyo_adf <- ur.df(food2$sampyo, type="drift")
summary(sampyo_adf)
# seoul의 비정상성 df 검증
seoul_adf <- ur.df(food2$seoul, type="drift")
summary(seoul_adf)
# sampyo와 seoul의 선형결합의 비정상성을 df 검증해본다
reg2 <- summary(lm(food2$sampyo ~ food2$seoul))
error2 <- residuals(reg2)
error_df2 <- ur.df(error2, type="none")
summary(error_df2)
return2 <- function(x) diff (x) / lag(x , -1) * 100
food_ret <- return2(food)
# Beta index
samlip_beta <- lm(food_ret$samplib ~ food_ret$kospi)$coefficient[[2]]
samyang_beta <- lm(food_ret$samyang ~ food_ret$kospi)$coefficient[[2]]
sampyo_beta <- lm(food_ret$sampyo ~ food_ret$kospi)$coefficient[[2]]
seoul_beta <- lm(food_ret$seoul ~ food_ret$kospi)$coefficient[[2]]
sinsegye_beta <- lm(food_ret$sinsegye ~ food_ret$kospi)$coefficient[[2]]
food_ts <- timeSeries(food_df, rownames(food_df))
FOOD_return <- returns(food_ts)
x11(); chart.CumReturns(FOOD_return, legend.loc = "topleft", main ="")
plot(portfolioFrontier(FOOD_return))
sampyo_beta
samplib_beta
samlib_beta
samlip_beta
samyang_beta
plot(portfolioFrontier(FOOD_return))
x11(); chart.CumReturns(FOOD_return, legend.loc = "topleft", main ="")
cardh <- read.csv("card_history.csv", header =T, stringsAsFactors = F)
par(mfrow = c(3,2))
plot(density(cardh$식료품), main = "식료품")
par
par(mfrow = c(3,2))
mfrow = c(3,2)
mfrow
par(mfrow)
cardh
par(mfrow)
par(mfrow = c(3,2))
par
plot(density(cardh$식료품), main = "식료품")
help(par)
plot(density(cardh$의복), main = "의복")
plot(density(cardh$외식비) , main = "외식비")
plot(density(cardh$책값) , main = "책값")
plot(density(cardh$온라인소액결제) , main = "온라인소액결제")
plot(density(cardh$의료비) , main = "의료비")
par(3,2)
help(par)
