import robotparser

sites = ['www.google.com' , 'www.offensive-security.com','www.yahoo.com']








def getDenies(site):
    paths = []

    # Create a robot parser instance and read the site's robots file

    robot = robot.parser.RobotFileParser()
    robot.set_url("http://"+site+"/robots.txt")
    robot.read()


    # For each entry, look at the rule lines and add the path to paths if
    # disallowed

    for entry in robot.entries:
        for line in entry.rulelines:
            not line.allowance and paths.append(line.path)


    return set(paths)


for site in sites:
    print "Denies for " + site
    print "\t" + " \n\t".join(getDenies(site))

    
